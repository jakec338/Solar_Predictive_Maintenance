
# Literature Review

\begin{table}[h]
\centering
\caption{kNN Regression Error Scores}
\label{tab:polyerror}
\begin{tabular}{|l|l|l|l|l|}
\hline
\rowcolor[HTML]{00171F}
\multicolumn{1}{|c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${E}_{RMS}$}} & \multicolumn{1}{c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${E}_{M}$}} & \multicolumn{1}{c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${E}_{\tilde{x}}$}} & \multicolumn{1}{c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${E}_{MP}$}} & \multicolumn{1}{c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${\sigma}^{2}$}} \\ \hline
0.7933                                                               & 0.6289                                                               & 0.5                                                             & 11.1712                                                           & 0.6280                                                           \\ \hline
\end{tabular}
\end{table}

## Solar Farm Design
\begin{align}
	\hat { y } \quad =\quad f({ x })\quad =\quad \frac { 1 }{ k } \sum _{ { x }_{ n }\in { â„• }_{ k }(x) }^{ k }{ { t }_{ n } } 
	\label{knn_func}
\end{align}

Photovoltaic (PV) cells are the basic unit of any PV installation that converts solar energy into electricity. To do this they require a material in which the absorption of a photon raises an electron to higher energy state. Crystalline Silicon wafer solar cells are by far the most common material for this purpose, constituting upwards of 90% of the global PV market \cite{MachineL13:online} (Chopra, 2004). They are rigid and, due to their mass production, the cheapest form of PV cell to produce. Thin film silicon is an alternative material, being more flexible than crystalline Si. There are three main types of thin film solar cells,  amorphous silicon (?-Si), copper indium gallium selenide (CIGS), and cadmium telluride (CdTe).

Solar farms are constituted of multiple PV modules. The hierarchy of different PV components is as follows. In a typical farm, PV cells are connected in a circuit to form a module. This module is then framed in glass and aluminium so as to protect the underlying cells from materials that might pollute them (dirt or exhaust fumes for example). Multiple modules are then mounted on a stable structure, resulting in a solar panel. A photovoltaic array is the complete power generating unit, consisting of any number of PV modules and mechanically independent panels. Panels must also be linked electrically. Multiple panels connected electrically are referred to as strings. Panels are connected in series to achieve a desired output voltage. 

All that is not PV module itself in a PV system is referred to as the Balance of Systems (BoS). Below the key components present in the BoS are detailed. 

Mounting System -
The mounting system permanently fixes an array to a stationary point. A tracking 	   motor may also be used in the mounting system that enables the modules to be moved to guarantee the maximum sun exposure over the course of the day. 

Solar Inverters - 
Inverters are components that are used to convert direct current (DC), generated by the solar cell into alternating current (AC) which is used by the grid for transmission to where it is needed. Inverters can be responsible for multiple modules or a single micro-inverter can be used fir an individual module. 

Storage- 
Storage is required if the PV system is to exist off grid or if there is a feed in tariff arrangement so that the system charges a battery. As battery technology progresses, the use of industrial scale battery storage is predicted to rise accordingly. 


## Predictive Maintenance

- Select number of folds for cross-validation and number of **k** values to test
- Create cross-validation folds given size of data **N**
- A dictionary of matrix of errors is created to hold the different error terms for each fold iteration
- For each fold separate the data into training and target sets
- Plug the training and target data sets into the **kNN regression function**
- **If k = 1:** For each row in the target set find the Euclidian/ Manhattan distance between it and each row of the training set. Store these distances in the columns of the 2-dimensional distances matrix. Create a matrix with corresponding training targets for each training row. Sort these two arrays based on increasing distance and store in the sort matrix
- Now there is a matrix that holds all the distances for all targets
- For each value of k take the first k values of the target column of the sort matrix and divide their sum by **k** 
- This is the kNN predicted target value which stored in the kNN_targets array
- Given the actual target values and kNN values compute the error values (RMSE, Mean, Median etc.) and store them in the error matrix
- Repeat this for all values of **k**
- Return the error matrix for that iteration of cross-validation
- Repeat until there are *K* number of error matrices for the *K* number of folds
- Take the average of the *K* matrices to get an error matrix with the average errors
- Find the k value for the minimum error of each error type	


## Results

\begin{figure}[H]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[trim = 0 0 0 0, clip, width=1\textwidth]{RMSvskNN.pdf}
 \caption{$E_{RMS}$ Variation with k}
 \label{fig:rmskNN}
\end{minipage}
\hfill
\begin{minipage}{.49\textwidth}
  \centering
   \includegraphics[trim = 0 0 0 0, clip, width=1\textwidth]{VariancevskNN.pdf}
   \caption{${\sigma}^{2}$ Variation with k}
  \label{fig:varkNN}
\end{minipage}
\vspace{-20pt}
\end{figure}

## Discussion

As seen on Figure \ref{fig:rmskNN} the kNN regression root mean square error is minimized at a k value of **18** using the Manhattan distance measure. This means that the method returns the most accurate results when distances are compared with the 18 nearest neighbours (Note: The k-value varies as tests are run due to the random nature of our cross-validation method). Lower values of **k** result in gross over-fitting as seen by the sharp increase in the error value.  

The kNN by nature has high-variance (\ref{fig:varkNN}) but that can be accounted for by increasing the number of neighbours affecting the prediction (**k**) as seen on the second graph above.

The model is simple to implement and gives immediate results but unfortunately is very resource intensive. Each time kNN is run it needs to iterate through each training row for every target then sort the data, which makes it both computionally and storage intensive (especially with large training data sets). Another major disadvantage is that it is a so-called 'lazy learner' as it does not return any generalized model which could be used for further testing.

