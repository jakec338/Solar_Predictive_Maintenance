
# Bayesian Polynomial Regression Model

\begin{table}[h]
\centering
\caption{Bayesian Polynomial Model Error Scores}
\label{tab:polyerror}
\begin{tabular}{|l|l|l|l|l|}
\hline
\rowcolor[HTML]{00171F}
\multicolumn{1}{|c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${E}_{RMS}$}} & \multicolumn{1}{c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${E}_{M}$}} & \multicolumn{1}{c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${E}_{\tilde{x}}$}} & \multicolumn{1}{c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${E}_{MP}$}} & \multicolumn{1}{c|}{\cellcolor[HTML]{00171F}{\color[HTML]{FFFFFF} ${\sigma}^{2}$}} \\ \hline
0.6423                                                               & 0.4984                                                               & 0.4036                                                             & 8.8329                                                           & 0.4111                                                           \\ \hline
\end{tabular}
\end{table}

## Overview
A Bayesian Model for linear regression aims to reduce the effect of over fitting, as well as provide a measure of the predictive distribution of the model. This is achieved by providing a prior probability distribution over the model parameters. The mean and covariance of this distribution along with the maximum likelihood estimates arrived at by linear model is used to calculate the posterior distributions. The posterior mean and covariance are given by the following equations respectively.

\begin{align}
    { m }_{ n }={ S }_{ N }({ S }_{ 0 }^{ -1 }{ m }_{ 0 }+\beta { \Phi  }^{ T }t) \\
    { S }_{ n }^{ -1 }={ S }_{ 0 }^{ -1 }+\beta { \Phi  }^{ T }\Phi
    \label{eq:polybasis}
\end{align}

The posterior covariance is used to derive a the predictive distribution for a given input, provided by the equation

\begin{align}
    { \sigma  }_{ N }^{ 2 }(x)=\frac { 1 }{ \beta  } +{ \Phi (x) }^{ T }{ S }_{ N }\Phi (x)
    \label{eq:polybasis}
\end{align}

 The predictive distribution can be used to compute the upper and lower bounds of the predicted values generated by the mean posterior weights. This can provide a graphical representation of the certainty of the model.

For the implementation of our bayesian model we used a polynomial model as the basis function to which it is applied. As the cubic polynomial model was the most accurate of our non-bayesian models, it was the natural choice.




## Method

- Select number of folds for cross-validation
- Create cross-validation folds given size of data **N**
- A dictionary of matrix of errors is created to hold the different error terms for each fold iteration
- For each fold separate the data into training and target sets
- Train and test inputs and targets are passed as parameters to the `polynomial_bayesian` function.
- The polynomial degrees and regularisation coefficient are given the optimum values ascertained by the polynomial model described in section `3`
- The `expand_to_2Dmonomials` function takes the training input data matrix and applies a polynomial basis function to the values. this returns an output matrix of each input value expanded within the range given by the polynomial degree value
- A prior mean weight, **m0** of 0 and an $\alpha$ of 100 are given
- The precision parameter $\beta$  is deduced by finding the variance of the raw data
- `calculate_weights_posterior` takes the output matrix training targets, $\beta$ , prior mean weights and covariance and returns the posterior mean weights and covariance
- Within a loop, these posterior values are then returned as prior value to the `calculate_weights_posterior` and used by the `construct_3dpoly` function to generate prediction values and predictive distribution values
- Error values for each iteration are generated by the `error_score` function
- The prediction and predictive distribution values returned on the final iteration are used to give the upper and lower bounds of the predicted values
- For all data folds, repeat steps $1-6$. After taking all the different cross-validation results from the model; these are aggregated to produce a final error score, prediction values, upper and lower bounds for the model


## Results

\begin{figure}[H]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[trim = 0 0 0 0, clip, width=1\textwidth]{RMSvspriors.pdf}
 \caption{$E_{RMS}$ Variation iterating calculated posterior values as priors}
 \label{fig:priors}
\end{minipage}
\hfill
\begin{minipage}{.49\textwidth}
  \centering
   \includegraphics[trim = 0 0 0 0, clip, width=1\textwidth]{predictive_distribution.pdf}
   \caption{Predictive distribution of values, plots upper bounds (blue), lower bounds (red) against predicted values}
  \label{fig:pred_dist}
\end{minipage}
\vspace{-20pt}
\end{figure}


### Discussion
As you can see from figure \ref{priors} the number of iterations through the `calculate_posterior_weights` function (where each posterior is then used as the prior in the next recursion) negatively effects the accuracy of the model. This implies that, although the accuracy of the polynomial model is improved marginally by the application of a prior, continual iterations generate weights that predict values that are farther from the actual target values.

This is particularly relevant when choosing our method of deriving a precision value. The precision value $\beta$ used in our final model was that derived from the variance of the raw target values. This is due to the other method of finding the $\beta$ being to find the variance of the residuals. This is dependent on predicted values that themselves are derived from posterior weights, therefore a $\beta$ value derived from them will become increasingly worse. 

Figure \ref{pred_dist} shows the predictive distribution of the model. This shows a largely constant $\sigma$ for the prediction value, consistent with the dense data that we received. There is some slight divergence at either end of the scale of values, but we can essentially say that we are equally confident of all our prediction values. 

Although removing features does increase the accuracy of the Bayesian Model to a significant extent as discussed above, this is not the case for the Bayesian Polynomial Regression model. Fewer features incorporated into the model reduces the problem of overfitting. This is reflected in the increased accuracy of the Polynomial Regression model, and is also the reason why there is no such improvement in the Bayesian model, as such over-fitting effects are mitigated already. 

The primary disadvantage of using the Bayesian approach lies in that it is always uncertain what value to choose as a prior. In our model we chose 0 as this is the standard in the literature when it is not obvious where the value should lie. This can result in final predictions being heavily influenced by the choice of prior

